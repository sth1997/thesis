\thusetup{
  %******************************
  % 注意：
  %   1. 配置里面不要出现空行
  %   2. 不需要的配置信息可以删除
  %******************************
  %
  %=====
  % 秘级
  %=====
  secretlevel={秘密},
  secretyear={10},
  %
  %=========
  % 中文信息
  %=========
  ctitle={面向稀疏矩阵的AllReduce优化技术研究},
  cdegree={工学硕士},
  cdepartment={计算机科学与技术系},
  cmajor={计算机科学与技术},
  cauthor={师天麾},
  csupervisor={翟季冬副教授},
  %cassosupervisor={陈文光教授}, % 副指导老师
  %ccosupervisor={某某某教授}, % 联合指导老师
  % 日期自动使用当前时间，若需指定按如下方式修改：
  % cdate={超新星纪元},
  %
  % 博士后专有部分
  catalognumber     = {分类号},  % 可以留空
  udc               = {UDC},  % 可以留空
  id                = {编号},  % 可以留空： id={},
  cfirstdiscipline  = {计算机科学与技术},  % 流动站（一级学科）名称
  cseconddiscipline = {系统结构},  % 专 业（二级学科）名称
  postdoctordate    = {2009 年 7 月——2011 年 7 月},  % 工作完成日期
  postdocstartdate  = {2009 年 7 月 1 日},  % 研究工作起始时间
  postdocenddate    = {2011 年 7 月 1 日},  % 研究工作期满时间
  %
  %=========
  % 英文信息
  %=========
  etitle={An Introduction to \LaTeX{} Thesis Template of Tsinghua University v\version},
  % 这块比较复杂，需要分情况讨论：
  % 1. 学术型硕士
  %    edegree：必须为Master of Arts或Master of Science（注意大小写）
  %             “哲学、文学、历史学、法学、教育学、艺术学门类，公共管理学科
  %              填写Master of Arts，其它填写Master of Science”
  %    emajor：“获得一级学科授权的学科填写一级学科名称，其它填写二级学科名称”
  % 2. 专业型硕士
  %    edegree：“填写专业学位英文名称全称”
  %    emajor：“工程硕士填写工程领域，其它专业学位不填写此项”
  % 3. 学术型博士
  %    edegree：Doctor of Philosophy（注意大小写）
  %    emajor：“获得一级学科授权的学科填写一级学科名称，其它填写二级学科名称”
  % 4. 专业型博士
  %    edegree：“填写专业学位英文名称全称”
  %    emajor：不填写此项
  edegree={Doctor of Engineering},
  emajor={Computer Science and Technology},
  eauthor={Xue Ruini},
  esupervisor={Professor Zheng Weimin},
  eassosupervisor={Chen Wenguang},
  % 日期自动生成，若需指定按如下方式修改：
  % edate={December, 2005}
  %
  % 关键词用“英文逗号”分割
  ckeywords={Allreduce, 稀疏矩阵, 分布式训练},
  ekeywords={Allreduce, Sparse Matrix, Distributed Training}
}

% 定义中英文摘要和关键字
\begin{cabstract}
  研究人员发现，参数较多的神经网络在进行分布式训练时性能不佳，其原因是分布式训练需要调用Allreduce算法来进行通信，受到网络带宽的限制，Allreduce算法速度较慢。本文的工作基于深度梯度压缩技术，该技术可以在不影响模型精度的情况下减少需要进行传输的参数数量，以此来减少通信时间。在深度梯度压缩技术的基础上，本文在以下几方面进行了进一步优化：
  \begin{itemize}
    \item 不再使用传统深度学习框架使用的环形Allreduce算法，改用蝶形Allreduce算法以降低通信延迟。
    \item 提出了稀疏矩阵的加法，使得在Allreduce每轮通信时稀疏矩阵可以直接进行加法，省去了压缩和解压缩的时间。
    \item 在选择第k大梯度时，不将梯度矩阵直接输入随机选择算法，而是先进行采样，将样本输入随机选择算法，再将结果作为阈值来筛选原梯度矩阵，将被筛选出的梯度再次输入随机选择算法，得到最终结果。
    \item 尝试使用智能网卡，在Allreduce的每轮通信中只是用智能网卡进行通信，稀疏矩阵的加法也在智能网卡上进行计算，主机端不参与其中，以此进一步降低通信延迟。
\end{itemize}

根据实验结果，本文提出的算法将第k大梯度选择的性能提升了46.0倍;与Open MPI的普通Allreduce性能相比，我们实现的SMPI通信库的稀疏矩阵Allreduce性能提升了38.0倍；将本文的优化移植至Pytorch后，Pytorch的分布式训练性能提高了3.3$\sim$8.6倍。
\end{cabstract}

% 如果习惯关键字跟在摘要文字后面，可以用直接命令来设置，如下：
% \ckeywords{\TeX, \LaTeX, CJK, 模板, 论文}

\begin{eabstract}
  Researchers found that the neural network with many parameters has poor performance in distributed training. The reason is that distributed training needs to call the Allreduce algorithm. Due to the limitation of network bandwidth, the Allreduce algorithm is slow. The work of this paper is based on Deep Gradient Compression, which can reduce the number of parameters that need to be transmitted without affecting the accuracy of the model, thus reducing the communication time. On the basis of Deep Gradient Compression, the following aspects are further optimized in this paper:
  \begin{itemize}
    \item Instead of using the Ring-Allreduce used by the traditional deep learning framework, Butterfly-Allreduce is used to reduce communication delay.
    \item We proposed the addition of sparse matrices, so that the sparse matrix can be added directly in each round of Allreduce communication, eliminating the time of compression and decompression.
    \item When selecting the k-th large gradient, the gradient matrix is not directly input into the Randomized-Selection algorithm, but is sampled first, the sample is input into the Randomized-Selection algorithm, then the result is used as a threshold to screen the original gradient matrix, and the screened gradients are input into the random selection method again to obtain the final result.
    \item Try to use Smart NICs. In each round of Allreduce communication, only Smart NICs are used for communication. The addition of sparse matrix is also calculated on Smart NICs, and the host side does not participate in it, so as to further reduce the communication delay.
\end{itemize}

According to the experimental results, the algorithm proposed in this paper improves the performance of the K-th large gradient selection by 46.0$\times$. According to the experimental results, the algorithm proposed in this paper improves the performance of the K-th large gradient selection by 46.0$\times$. With the optimization in this paper, Pytorch's distributed training performance is improved by 3.3$\times \sim$8.6$\times$.

\end{eabstract}

% \ekeywords{\TeX, \LaTeX, CJK, template, thesis}
