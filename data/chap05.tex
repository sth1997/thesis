\chapter{结论}
\label{chap5}

\section{本文的研究内容和结论}
本文以深度梯度压缩技术为基础，并在稀疏矩阵的加法、Allreduce算法选择、第k大梯度选择以及智能网卡的使用等多方面进行优化，最终将稀疏矩阵Allreduce过程性能提升至Open MPI Allreduce过程性能的38.0倍，并将优化了Allreduce过程的Pytorch分布式训练性能提升至原来的8.6倍，这使得即使在使用带宽较低的1000 Mbps Ethernet时，用了本文的算法进行优化，得到的性能依然可以比肩甚至超越使用带宽较高的100 Gbps Infiniband时的原算法性能。同时，本文的算法具有非常好的可扩展性。

具体地，深度梯度压缩技术可以在不影响模型精度的情况下，将分布式训练中需要进行传输的参数量但幅度减少，但是传统的深度学习框架中，分布式训练使用的Allreduce算法都是环形算法，它的通信总量非常少，但是通信轮数较多，并不适合通信量少的情况。所以本文选择使用蝶形Allreduce算法，它的通信轮数非常少，并且由于需要通信的参数量非常少，所以通信总量并不大。

深度梯度压缩技术提出了将梯度矩阵压缩为稀疏矩阵后再进行传输的思想，若在Allreduce算法的每一轮都进行压缩、传输再解压缩并进行相加，就会耗费太多时间。本文提出了一种稀疏矩阵直接相加的方法，使用本方法就可以在每轮通信时省去压缩和解压缩的时间。

同时，深度梯度压缩技术需要在梯度矩阵中选择出绝对值最大的k个梯度，这就需要调用随机选择算法来找到第k大梯度的值，并用这个值对原梯度矩阵进行筛选并压缩为稀疏矩阵。但是随机选择算法的速度非常慢，本文的算法会先对原梯度矩阵进行采样，只将部分样本输入进随机选择算法，再结果当做阈值对原梯度矩阵进行筛选，将筛选出来的部分梯度再次输入进随机选择算法的到最终结果。这个优化可以在单线程和多线程下分别将第k大梯度的选择过程的速度提高13.3倍和46.0倍。

最后，本文尝试使用智能网卡，使得Allreduce每轮通信过程只在各节点的网卡之间进行，并在网卡上进行稀疏矩阵的计算，主机端并不参与其中，以此来进一步降低延迟。

根据实验结果，在使用TCP进行通信时，我们实现的SMPI通信库的稀疏矩阵Allreduce性能比Open MPI的普通Allreduce性能高38.0倍。对Pytorch的分布式训练进行优化后，本文选择的几个常用的神经网络的分布式训练性能提高了3.3$\sim$8.6倍。

\section{进一步的工作}
本文实现的SMPI通信库以及移植Gloo的工作目前只支持CPU，并不支持GPU。如前文所说，神经网络的计算--通信比越小，优化Allreduce过程后得到的性能提升就越高。所以，之后若将本文的优化算法移植至GPU，相信可以为神经网络分布式训练带来更高的性能提升。

同时，由于硬件原因，在智能网卡的实验中，我们只使用了两个节点，无法证明智能网卡能够降低Allreduce中每轮通信的延迟。希望在未来的工作中能够完成这部分实验。