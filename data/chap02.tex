\chapter{相关研究综述}
\label{chap2}

\section{神经网络分布式训练}
目前神经网络在计算机视觉、自然语言处理与语音识别等多方面有着广泛的应用并取得了良好的效果，但是训练神经网络所需的计算量很大，对计算设备有着很高的要求。一台计算设备的计算能力有限，在面对参数较多的神经网络的训练时，训练时间会变得很长。为了加快神经网络训练速度，就需要使用多台计算设备进行分布式训练。

%可以放一个数据并行和模型并行的图，类似于https://blog.csdn.net/qq_35799003/article/details/84981009
神经网络主要有两种并行计算的方法，分别是数据并行和模型并行：
\begin{itemize}
    \item 数据并行是指每台机器上都有模型的一份副本，在每轮训练开始前，各个机器上的模型参数都是相同的。在每一轮训练中，不同机器获取数据集中不同的数据作为神经网络的输入，将各个机器的结果进行组合，更新各机器上的模型参数，并进行下一轮训练。
    \item 模型并行是指每台机器上都有一部分模型，不同机器负责不同部分的模型的计算。比如，以层为单位将神经网络分割并分配给不同的机器。
\end{itemize}

一般而言，数据并行是分布式训练的首选并行计算方法，因为数据并行不仅仅更容易实现，而且拥有更高的性能。因为神经网络的训练过程中数据有将强的依赖性，后面的层需要以前面的层的结果作为输入，所以模型并行的性能会稍差一些。本文主要关注数据并行。

数据并行的分布式训练需要在各个机器上的模型之间同步参数，其方法有很多种，我们只介绍最常见的一种同步方式---使用Allreduce算法进行模型参数的同步。

使用Allreduce算法进行模型参数同步的分布式训练大致过程如下：

\begin{itemize}
    \item [1)]
    每台机器上都启动分布式训练进程，对所有模型参数进行初始化。可以使用相同的随机种子与相同初始化方法来保证个机器中的模型初始化参数相同，也可以将某台机器中的进程作为主进程，由主进程初始化梯度，并通过Broadcast\footnote{Broadcast是MPI的一个原语（MPI\_Bcast），可以从一个进程读取数据，并且广播给通信域中的其他所有进程。}将参数广播给其他机器。
    \item [2)]
    每台机器从数据集中读取不同的数据作为输入。
    %可以放正向传播和反向传播的图片
    \item [3)]
    执行正向传播算法。神经网络的每一层根据当前层的输入进行计算，并将计算结果输出给之后的层。直到最后一层，算出最终结果，并与标定好的结果一起输入Loss Function（损失函数），得到Loss。
    \item [4)]
    执行反向传播算法。根据Loss，由深层至浅层，逐层计算出每个参数的梯度。
    \item [5)]
    调用Allreduce过程，将所有机器中参数的梯度逐元素求和并除以进程数以得到平均值。
    \item [6)]
    根据每个参数的平均梯度来对参数进行优化。由于各机器调用Allreduce后参数的平均梯度是相同的，调用的参数优化算法也是相同的，所以各机器间优化后的模型参数也依然是相同的。
    \item [7)]
    重复过程2)$\sim$6)的训练过程，直到手动终止训练或到达预设的训练轮次数。
\end{itemize}

相对于使用一台机器进行训练，分布式训练确实可以加速模型收敛，但是它的并行效率并不高，因为分布式训练需要进行Allreduce过程，而对于参数比较多的大模型而言，所有参数都需要输入Allreduce算法进行通信，通信开销会非常大~\cite{li2014communication, wen2017terngrad}。