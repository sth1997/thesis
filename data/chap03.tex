\chapter{一种面向稀疏矩阵的Allreduce优化技术}
\label{chap3}

\section{动机}

随着神经网络技术的不断发展，尤其是Resnet（引用）与BERT（引用）网络的出现，深度神经网络的设计逐渐向着网络更深且参数更多的方向发展，因为这样的网络可以得到更高的精度。但是随着网络变大，训练网络所需要的时间也就变得更长。

为了缩短训练时间，就需要使用多块显卡同时训练，甚至需要用多台机器进行分布式训练。由于深度神经网络训练的特殊性，导致其计算与通信难以重叠，网络带宽也就成为了深度神经网络分布式训练的一处瓶颈。随着多台计算设备的引入，同一个网络在相同训练集上的计算速度可以得到很大的提升，但不幸的是，神经网络相关的数据（尤其是梯度）在显卡之间以及机器之间相互传输花费了大量的时间，分布式训练的并行效率并不高，尤其是循环神经网络（RNN），它的计算-通信比非常低。在网络环境较差的条件下（比如一些实验室只有以太网），深度神经网络的分布式训练的计算-通信比甚至低于50\%。

为了减少深度神经网络分布式训练中通信的开销，最近几年有很多新算法与新网络被提出，包括梯度压缩（引用），深度梯度压缩（引用），以及一些与联合学习有关的算法（引用）等。这些算法与网络的核心思想都是在不改变原深度神经网络结构且保持精度不下降（或稍微下降）的前提下，减少分布式训练中需要传输的通信量。

深度梯度压缩技术通过只选取深度神经网络反向传播算法所得到的梯度中的最重要的——即绝对值最大的——0.1\%的参数进行传输，也能够保持最终神经网络训练所得到的精度基本不变。得益于通信量的大幅度减少，分布式训练的时间也随之减少，训练速度得到一定提升。
但是深度梯度压缩技术的实现还有一些方面存在一定问题，具有继续优化的空间：

\subsection{环形\texttt{Allreduce}算法的性能问题}
  在大多数深度学习网络框架内，分布式训练在通信阶段使用的Allreduce算法为环形算法（比如Pytorch的官方后端Gloo中所使用的就是环形Allreduce算法）。
  
  在之前（引用）我们分析过，环形算法拥有总通信量非常小的优点，但是相对的，其通信轮数非常多。在深度神经网络参数较多、通信量较大时，环形算法确实有着非常大的优势。但是若通信量较小，那么环形算法就无法体现出自己的优势，因为此时通信的瓶颈不在网络带宽，而在网络延迟，又由于环形算法的通信轮数较多，引入了较大的延迟，反而会不如其他Allreduce算法。由于环形算法的通信轮数与节点数成正相关，所以节点数越多时，环形算法在通信量较小的传输中的劣势也就越大，在之后的测试中（引用）我们也能看出，在节点数较多时环形算法在性能上存在一定问题。
  
  所以对于使用了深度梯度压缩技术的深度神经网络分布式训练，使用传统深度学习框架后端的环形Allreduce算法并不合适。

\subsection{稀疏矩阵压缩与解压缩的开销}
使用深度梯度压缩技术后，只需要将梯度中0.1\%参数进行通信。梯度矩阵只有0.1\%的非零元，非常稀疏，所以会进行压缩并采用稀疏矩阵的格式进行传输。

在Allreduce操作中，每完成一轮传输，都需要用接收到的数据与本地数据进行Reduce操作，即按位将两者的每一位分别相加。但由于我们采用了稀疏矩阵的格式，无法直接按位相加。

若我们选择在每一轮接收到稀疏矩阵格式的数据后，先将其解压缩为一个普通矩阵，再进行按位的Allreduce操作，之后再压缩为稀疏矩阵，再进行下一轮的传输，这样会非常的浪费时间。如表（这里需要插入一个表格），在对一个大小为2GB的矩阵（512M个单精度浮点数）进行一轮的Allreduce操作中，（应该在表格注明单线程）压缩、解压缩与Reduce操作各需要xxxx秒、xxxx秒和xxxx秒，而稀疏矩阵通信仅需要xxxx，大部分的时间都用在压缩、解压缩与Reduce操作。若我们能设计一种算法，使得Reduce操作可以在稀疏矩阵格式下直接进行，就可以省去每轮中压缩与解压缩的时间，同时，也可以省去按位相加时大量没有意义的零元加法的时间。

\subsection{设备间的传输延迟}
（这里插入一张图）如图是使用普通网卡，每一轮通信都需要先由一台机器将稀疏矩阵由内存发送至网卡，网卡再通过网络（也许中间会经过交换机等设备）发送至其他机器的网卡，其他机器再将数据由网卡发送至内存。

根据上一节的分析，若我们可以在稀疏矩阵格式下直接进行Reduce操作，其计算量是非常小的，Allreduce的瓶颈主要在各设备之间的通信延迟。由于计算量较小，我们并不需要计算性能很高的CPU或GPU设备，所以在接下来我们设计的算法中使用智能网卡。智能网卡的引入可以避免每轮传输中机器内部从内存到网卡的两次传输延迟，且由于Reduce操作计算量较小，即使智能网卡上的CPU性能没有主机端的CPU性能高，也并不会导致产生过多的额外计算时间。

\section{算法设计}
\subsection{稀疏矩阵存储格式与稀疏矩阵的加法}
在前文中（引用）我们介绍过多种稀疏矩阵的存储格式，但它们都不太适合稀疏的神经网络梯度矩阵的通信。在大多数

\subsection{\texttt{Allreduce}算法选择}